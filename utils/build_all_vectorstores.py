# build_all_vectorstores.py
# âœ… æ„å»ºå¤šä¸ªå‘é‡æ•°æ®åº“ï¼ˆæ¯ä¸ªå­¦ç§‘ä¸€ä¸ª vectorstore å­ç›®å½•ï¼‰

import os
import json
from langchain_community.vectorstores import FAISS
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# âœ… è‡ªåŠ¨å®šä½é¡¹ç›®è·¯å¾„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# âœ… è¾“å…¥ JSON è·¯å¾„ï¼ˆæ¥è‡ª PDF æå–ï¼‰
PDF_ROOT = os.path.join(BASE_DIR, "..", "data", "pdf_output")

# âœ… è¾“å‡º FAISS å‘é‡æ•°æ®åº“è·¯å¾„
VECTORSTORE_ROOT = os.path.join(BASE_DIR, "..", "data", "vectorstore")

# âœ… åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# âœ… éå†æ¯ä¸ªè¯¾ç¨‹ç›®å½•
for subject in os.listdir(PDF_ROOT):
    subject_path = os.path.join(PDF_ROOT, subject)
    if not os.path.isdir(subject_path):
        continue

    all_docs = []
    for file in os.listdir(subject_path):
        if file.endswith(".json"):
            file_path = os.path.join(subject_path, file)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)

                if not isinstance(data, dict):
                    continue

                for page, content in data.items():
                    if content.strip():
                        all_docs.append(Document(
                            page_content=content,
                            metadata={
                                "domain": subject,
                                "filename": file,
                                "page": page
                            }
                        ))
                print(f"âœ… åŠ è½½ï¼š{file_path}")
            except Exception as e:
                print(f"âŒ é”™è¯¯ï¼š{file_path}ï¼ŒåŸå› ï¼š{e}")

    if not all_docs:
        print(f"âš ï¸ è·³è¿‡ï¼š{subject}ï¼ˆæ— æœ‰æ•ˆæ–‡æ¡£ï¼‰")
        continue

    # âœ… åˆ‡åˆ†æ–‡æœ¬ & æ„å»ºæ•°æ®åº“
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_docs = splitter.split_documents(all_docs)

    # âœ… è¾“å‡ºç›®å½•ç»Ÿä¸€ä¸º group_project/data/vectorstore/{subject}
    subject_vectorstore_path = os.path.join(VECTORSTORE_ROOT, subject)
    os.makedirs(subject_vectorstore_path, exist_ok=True)

    db = FAISS.from_documents(split_docs, embedding)
    db.save_local(subject_vectorstore_path)

    print(f"âœ… æ„å»ºå®Œæˆï¼š{subject}ï¼Œä¿å­˜åˆ° {subject_vectorstore_path}ï¼Œå…± {len(split_docs)} å—")

print("ğŸ‰ æ‰€æœ‰å‘é‡æ•°æ®åº“å·²æ„å»ºå®Œæˆï¼")